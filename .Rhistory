##0.4 represents that now it's in the top %1
##Lastly i is the value that is scaled by the probability of the action happening i.e. 0.6^(i-1)*0.4
for(i in 1:100){
expected_val <- expected_val +0.6^(i-1)*0.4*i
}
expected_val
cl <- kmeans(scale(npf[,vars]),centers=kmeansppcenters(scale(npf[,vars]),4),algorithm="Lloyd",iter.max=100)
print(min(losses))
print(max(losses))
print(cl$tot.withinss)
library(clue)
losses <- rep(0,1000)
for(i in 1:1000){
losses[i] <- kmeans(scale(npf[,vars]),centers=4,algorithm="Lloyd",iter.max=100)$tot.withinss
}
hist(losses)
hist(losses, breaks=100)
print(min(losses))
print(max(losses))
expected_val <- 0
## Expected value can be calculated by taking the ratio of losses in top 1% as the probability to randomly obtain a loss in the top 1%
## It seems that the ratio is approximately somewhere 0.4, seems to be more but don't have to be that precise.
## Now with the formula for the expected value of distinct random values we can calculate the expected value of the first loss in the top %1
##Probability is 0.6(i-1)*0.4*i.
##0.6^(i-1) represents that earlier losses weren't in the top %1
##0.4 represents that now it's in the top %1
##Lastly i is the value that is scaled by the probability of the action happening i.e. 0.6^(i-1)*0.4
for(i in 1:100){
expected_val <- expected_val +0.6^(i-1)*0.4*i
}
expected_val
losses <- rep(0,1000)
for(i in 1:1000){
losses[i] <- kmeans(scale(npf[,vars]),centers=kmeansppcenters(scale(npf[,vars]),4),algorithm="Lloyd",iter.max=100)$tot.withinss
}
hist(losses,100)
print(min(losses))
print(max(losses))
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
complete.labels <- cuttree(hc.complete, 4)
single.labels <- cuttree(hc.complete,2)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
complete.labels
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt <- tt[,solve_LSAP(tt,maximum=TRUE)]
tt
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt,maximum=TRUE)]
tt.complete
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(npf[,vars]), method="complete")
hc.single <- hclust(dist(npf[,vars]), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt,maximum=TRUE)]
tt.complete
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt,maximum=TRUE)]
tt.complete
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=FALSE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
tt.single
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
print(tt.single)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt.complete,maximum=TRUE)]
print(tt.complete)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
print(tt.single)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt,maximum=TRUE)]
print(tt.complete)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
average.labels <- cuttree(hc.mean, 4)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
average.labels <- cutree(hc.mean, 4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
print(tt.single)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt.complete,maximum=TRUE)]
print(tt.complete)
tt.average <- table(npf$class4,average.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.average <- tt.average[,solve_LSAP(tt.average,maximum=TRUE)]
print(tt.average)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
plot(hc.average)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
plot(hc.single)
plot(hc.mean)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
average.labels <- cutree(hc.mean, 4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
print(tt.single)
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt.complete,maximum=TRUE)]
print(tt.complete)
tt.average <- table(npf$class4,average.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.average <- tt.average[,solve_LSAP(tt.average,maximum=TRUE)]
print(tt.average)
hc.complete <- hclust(dist(scale(npf[,vars])), method="complete")
hc.single <- hclust(dist(scale(npf[,vars])), method="single")
hc.mean <- hclust(dist(scale(npf[,vars])), method="average")
plot(hc.complete)
## As can be seen small number of observations are fused one-at-a-time as is mentioned in course book.
plot(hc.single)
plot(hc.mean)
complete.labels <- cutree(hc.complete, 4)
single.labels <- cutree(hc.complete,4)
average.labels <- cutree(hc.mean, 4)
tt.single <- table(npf$class4,single.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.single <- tt.single[,solve_LSAP(tt.single,maximum=TRUE)]
print(tt.single)
##outlier is in it's own cluster
tt.complete <- table(npf$class4,complete.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.complete <- tt.complete[,solve_LSAP(tt.complete,maximum=TRUE)]
print(tt.complete)
tt.average <- table(npf$class4,average.labels)
## Find a permutation of cluster indices such that they
## best match the classes in class4.
tt.average <- tt.average[,solve_LSAP(tt.average,maximum=TRUE)]
print(tt.average)
npf <- read.csv("npf_train.csv")
## choose variables whose names end with ".mean" together with "class4"
vars <- colnames(npf)[sapply(colnames(npf),
function(s) nchar(s)>5 && substr(s,nchar(s)-4,nchar(s))==".mean")]
npf <- npf[,c(vars,"class4")]
## strip the trailing ".mean" to make the variable names prettier
colnames(npf)[1:length(vars)] <- sapply(colnames(npf)[1:length(vars)],
function(s) substr(s,1,nchar(s)-5))
vars <- colnames(npf)[1:length(vars)]
losses <- rep(0,20)
for(i in 1:20){
losses[i] <- kmeans(scale(npf[,vars]),centers=i,algorithm="Lloyd",iter.max=100)$tot.withinss
}
plot(losses, ylab="Loss", xlab="k")
losses <- rep(0,20)
for(i in 1:20){
losses[i] <- kmeans(npf[,vars],centers=i,algorithm="Lloyd",iter.max=100)$tot.withinss
}
plot(losses, ylab="Loss", xlab="k")
##Normalizing the data reduces the magnitude of the losses.
##This happens because the distances between points is smaller when points features are
npf <- read.csv("npf_train.csv")
print(scale(npf))
print(scale(npf[,vars]))
##Normalizing the data reduces the magnitude of the losses.
##This happens because the distances between points is smaller when points features are
npf <- read.csv("npf_train.csv")
npf <- npf[,c(vars,"class4")]
## strip the trailing ".mean" to make the variable names prettier
colnames(npf)[1:length(vars)] <- sapply(colnames(npf)[1:length(vars)],
function(s) substr(s,1,nchar(s)-5))
vars <- colnames(npf)[1:length(vars)]
scale(npf[,vars])
pr.out <- prcomp(npf[,vars], scale=TRUE)
##Normalizing the data reduces the magnitude of the losses.
##This happens because the distances between points is smaller when points' features are scaled to the same scale
## This is beneficial since with a loss of smaller magnitude it can be easier to choose which k should be picked
npf <- read.csv("npf_train.csv")
## choose variables whose names end with ".mean" together with "class4"
vars <- colnames(npf)[sapply(colnames(npf),
function(s) nchar(s)>5 && substr(s,nchar(s)-4,nchar(s))==".mean")]
npf <- npf[,c(vars,"class4")]
## strip the trailing ".mean" to make the variable names prettier
colnames(npf)[1:length(vars)] <- sapply(colnames(npf)[1:length(vars)],
function(s) substr(s,1,nchar(s)-5))
vars <- colnames(npf)[1:length(vars)]
losses <- rep(0,20)
for(i in 1:20){
losses[i] <- kmeans(scale(npf[,vars]),centers=i,algorithm="Lloyd",iter.max=100)$tot.withinss
}
plot(losses, ylab="Loss", xlab="k")
losses <- rep(0,20)
for(i in 1:20){
losses[i] <- kmeans(npf[,vars],centers=i,algorithm="Lloyd",iter.max=100)$tot.withinss
}
plot(losses, ylab="Loss", xlab="k")
pr.out <- prcomp(npf[,vars], scale=TRUE)
pr.out <- prcomp(npf[,vars], scale=TRUE)
pr.out
pr.out <- prcomp(npf[,vars], scale=TRUE)
biplot(pr.out, scale = 0)
pr.out <- prcomp(npf[,vars], scale=TRUE)
biplot(pr.out[1:2], scale = 0)
pr.out <- prcomp(npf[,vars], scale=TRUE)
biplot(pr.out[1:2,], scale = 0)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
print(npf)
summary(npf)
set.seed(7)
# load caret
library(caret)
install.packages("caret")
set.seed(7)
# load caret
library(caret)
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
plot(model)
plot(varImp(model, scale=FALSE), ylab="Variables")
print(importance)
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
model1 <- train(class2~., data=npf75, method="svmLinear2", preProcess="scale", trControl=control)
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
plot(model)
plot(varImp(model, scale=FALSE), ylab="Variables")
print(importance)
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
npf82 <- npf[, c("class2",npf82names)]
npf83_8 <- npf[, c("class2",npf83_8names)]
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
print(importance)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf_test
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf_test
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf_test
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
plot(model)
plot(varImp(model, scale=FALSE), ylab="Variables")
print(importance)
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))] #move class2 column to first
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75 <- npf[, c("class2",npf75names)]
npf75 <- npf[, c("class2")]
npf75names <- importance[importance["event"]>0.75,]$names #33 features
npf82names <- importance[importance["event"]>0.80,]$names # 12
npf83_8names <- importance[importance["event"]>0.838,]$names #6
npf75names
c("class2",npf75names)
head(npf)
npf75name
npf75names
c("class2",npf75names)
npf75names
c("class2",npf75names)
npf75 <- npf[, npf75names]
knitr::opts_chunk$set(echo = TRUE)
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="ordinalRF", preProcess="scale", trControl=control)
