npf <- data.frame(c(1,1,1))
npf
npf <- data.frame(c(1,1,1), c(2,2,2))
npf
npf <- npf[-1]
npf
npf <- data.frame(c(1,1,1), c(2,2,2), c(3,3,3))
npf <- npf[-(1:2)]
npf
npf <- read.csv("npf_train.csv")
npf <- read.csv("npf_train.csv")
npf <- read.csv("Documents/npf_train.csv")
npf
npf <- npf[-(1:2)]
head(npf)
npf <- read.csv("npf_train.csv")
npf <- read.csv("Documents/npf_train.csv")
npf <- read.csv("npf_train.csv")
npf <- npf[-(1:2)]
npf
my_func <- function(x){
if(x=="nonevent"){
return(x)
}
return("event")
}
npf <- read.csv("npf_train.csv")
npf <- npf[-(1:2)]
npf$class4 <- sapply(npf$class4, my_func)
npf$class4
npf$class4
npf <- read.csv("npf_train.csv")
npf <- npf[-(1:2)]
npf$class4 <- sapply(npf$class4, my_func)
glm.fit <- glm(class4 ~ ., data=npf, family=binomial)
my_func <- function(x){
if(x=="nonevent"){
return(x)
}
return("event")
}
npf <- read.csv("npf_train.csv")
npf <- npf[-(1:2)]
npf$class4 <- as.factor(sapply(npf$class4, my_func))
glm.fit <- glm(class4 ~ ., data=npf, family=binomial)
summary(glm)
summary(glm.fit)
importance <- varImp(glm.fit, scale=FALSE)
library(caret)
my_func <- function(x){
if(x=="nonevent"){
return(x)
}
return("event")
}
npf <- read.csv("npf_train.csv")
npf <- npf[-(1:2)]
npf$class4 <- as.factor(sapply(npf$class4, my_func))
glm.fit <- glm(class4 ~ ., data=npf, family=binomial)
importance <- varImp(glm.fit, scale=FALSE)
importance
importance <- sort(importance)
importance
importance <- order(importance)
print(importance)
importance <- varImp(glm.fit, scale=FALSE)
print(importance)
importance$Overall <- sort(importance$Overall)
head(importance)
importance <- varImp(glm.fit, scale=FALSE)
importance$Overall <- sort(importance$Overall, decreasing = TRUE)
head(importance)
plot(importance$Overall[:5])
plot(importance$Overall[1:5])
plot(importance[1:5])
plot(importance[1:5,])
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf_test
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
plot(model)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf_test <- read.csv("npf_test_hidden.csv")
npf_test
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf_test <- npf_test[,-c(1:2,4)]
print(npf)
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
plot(model)
predict(model, data=npf_test)
mean(npf_test$class2 == predict(model, data=npf_test, type="prob"))
prediction <- predict(model, data=npf_test, type="prob")
```{r}
prediction$nonevent
summary(prediction)
prediction <- predict(model, data=npf_test, type="prob")
table(predict = prediction, truth = npf_test$class2)
prediction <- predict(model, data=npf_test, type="prob")
print(prediction)
table(predict = prediction, truth = npf_test$class2)
npf_test$class2
npf$class2
set.seed(7)
# lets use caret
library(caret)
#note that classprobs argument has to be true if we want svm to give probabilities
control <- trainControl(method="repeatedcv", number=10, repeats=3,classProbs=TRUE)
model <- train(class2~., data=npf, method="svmLinear2", preProcess="scale", trControl=control)
#lets sort the importances
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
importance <- importance[order(importance$event,decreasing = T),]
prediction <- predict(model, data=npf_test, type="prob")
plot(model)
library(caret)
get_non_correlated_features <- function(path, cutoff){
npf <- read.csv("npf_train.csv") #read in data
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
npf$class2 <- factor("event",levels=c("nonevent", "event"))
npf$class2[npf$class4=="nonevent"] <- "nonevent"
npf <- npf[,-1]
npf <- npf[,c(ncol(npf),1:(ncol(npf)-1))]
cm <- cor(npf[,-1]) # note that we ignore the class 2 column here as we dont want to drop it obviously
# find attributes that are highly corrected (the threshold for being cut can be changed.)
highcorr <- findCorrelation(cm, cutoff = cutoff)+1
npf <- npf[,-c(highcorr)]
return(npf)
}
get_importance <- function(model){
plot(varImp(model, scale=FALSE), ylab="Variables")
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance <- data.frame(importance, names = rownames(importance))
feature_importance <- importance$event
if (is.null(feature_importance)){
feature_importance <- importance$Overall
}
importance <- importance[order(feature_importance,decreasing = T),]
return(importance)
}
train_with_different_features <- function(model_name, corr_cutoff){
npf <- get_non_correlated_features("npf_train.csv", corr_cutoff)
control <- trainControl(method="repeatedcv", number=10, repeats=5,classProbs=TRUE)
model <- train(class2~., data=npf, method=model_name, preProcess="scale", trControl=control)
importance <- get_importance(model)
models <- list()
accuracy <- rep(0,25)
for(i in 1:length(importance)){
npfnames <- sapply(importance$names[1:i], function(x) toString(x))
npftrain <- npf[, c("class2",npfnames)]
model1 <- train(class2~., data=npftrain, method=model_name, preProcess="scale", trControl=control)
models <- list(models, model1)
}
return(models[-1])
}
plot_accuracies <- function(models){
n <- length(models)
accuracy <- rep(0,n)
for(i in 1:n){
print(i)
accuracy[i] <- sapply(models[[i]]$result, mean)[2]
}
print(accuracy)
plot(accuracy)
}
train_with_different_cutoff <- function(model_name, cutoffs){
models <- list()
for(cutoff in cutoffs){
models <- list(models, train_with_different_features(model_name, cutoff))
}
return(models)
}
models <- train_with_different_features("LogitBoost", c(0.75))
