## this allows us to hide all of the details in function cv...
test=rmse(cv(rbind(data_test,data_train),
split=list(1:nrow(data_test),
(nrow(data_test)+1):(nrow(data_test)+nrow(data_train))),
model=model)[1:nrow(data_test)],data_test[,"medv"]),
## we finally do the regular 10-fold cross-validation
CV=rmse(cv(data_train,split=split,model=model),data_train[,"medv"]))
})
knitr::kable(t(a),"simple",digits=3)
library(MASS)
library(rpart)
library(randomForest)
library(e1071)
set.seed(42)
idx <- sample.int(nrow(Boston),100)
print(idx)
data_train <- Boston[ idx,] # train using this data
print(data_train)
data_test <- Boston[-idx,] # holdout data for final analysis
print(data_test)
#' root mean squared error measure
rmse <- function(yhat,y) sqrt(mean((y-yhat)**2))
#' split n items into k random folds of roughly equal size
kpart <- function(n,k) {
p <- sample.int(n) # random permutation of integers from 1 to n
s <- ((0:k)*n) %/% k # "boundaries" between k folds
mapply(function(a,b) p[a:b],s[-k-1]+1,s[-1],SIMPLIFY=FALSE)
}
#' "normalize" column names to X1, X2, X3, etc.
normcolnames <- function(X) {
colnames(X) <- sapply(1:ncol(X),function(i) sprintf("X%d",i))
X
}
#' Find cross validation predictions
cv <- function(data=Boston, # defaults to Boston data
target="medv", # default target variable
## separate covariates...
data_x=data[,setdiff(colnames(data),target),drop=FALSE],
## ...and dependent variable
data_y=data[,target],
## "normalize" column names of covariates to X1, X2, ...
X=normcolnames(data_x),
XY=data.frame(X,Y=data_y), # rename dependent variable as Y
n=nrow(X), # number of rows in the data matrix
k=min(n,10), # number of cross-validation folds
split=kpart(n,k), # the split of n data items into k folds
model=lm, # model to use
## function to train a model on data XY
train=function(XY) model(Y ~ .,data=XY),
## function to make predictions on trained model
pred=function(m,X) predict(m,newdata=X),
## helper function to train a model in datapoints in itr and
### make a prediction on datapoints in iva
f=function(itr,iva) pred(train(XY[itr,,drop=FALSE]),X[iva,,drop=FALSE])) {
## initialize yhat to something of correct data type,
## train model with full data and make a prediction
yhat <- f(1:n,1:n)
if(k>0) {
## go through all folds, train on other folds, and make a prediction
for(iva in split) {
yhat[iva] <- f(setdiff(1:n,iva),iva)
}
}
yhat # finally, output cross-validation predictions
}
## Some regression models implemented in R. For documentation, just type
## ?lm, ?rpart etc. Notice that you need the above mentioned libraries to be
## able to use these models.
models <- list(OLS=lm,
regression_tree=rpart,
random_forest=randomForest,
SVM=svm)
## use the same random split for all - this eliminates variance due
### to different splits for different models
split <- kpart(nrow(data_train),10)
a <- sapply(models,function(model) {
## with k=0 we just get predictions on training data
c(
## with k=0 we just get predictions on training data
train=rmse(cv(data_train,k=0,model=model),data_train[,"medv"]),
## we concatenate test and train data and have them on separate folds,
## after which we get the fold out where
## we trained on training data and predicted on testing data.
## Looks ugly, but
## this allows us to hide all of the details in function cv...
test=rmse(cv(rbind(data_test,data_train),
split=list(1:nrow(data_test),
(nrow(data_test)+1):(nrow(data_test)+nrow(data_train))),
model=model)[1:nrow(data_test)],data_test[,"medv"]),
## we finally do the regular 10-fold cross-validation
CV=rmse(cv(data_train,split=split,model=model),data_train[,"medv"]))
})
knitr::kable(t(a),"simple",digits=3)
library(MASS)
library(rpart)
library(randomForest)
library(e1071)
set.seed(42)
idx <- sample.int(nrow(Boston),100)
print(idx)
data_train <- Boston[ idx,] # train using this data
View(data_train)
data_test <- Boston[-idx,] # holdout data for final analysis
print(data_test)
#' root mean squared error measure
rmse <- function(yhat,y) sqrt(mean((y-yhat)**2))
#' split n items into k random folds of roughly equal size
kpart <- function(n,k) {
p <- sample.int(n) # random permutation of integers from 1 to n
s <- ((0:k)*n) %/% k # "boundaries" between k folds
mapply(function(a,b) p[a:b],s[-k-1]+1,s[-1],SIMPLIFY=FALSE)
}
#' "normalize" column names to X1, X2, X3, etc.
normcolnames <- function(X) {
colnames(X) <- sapply(1:ncol(X),function(i) sprintf("X%d",i))
X
}
#' Find cross validation predictions
cv <- function(data=Boston, # defaults to Boston data
target="medv", # default target variable
## separate covariates...
data_x=data[,setdiff(colnames(data),target),drop=FALSE],
## ...and dependent variable
data_y=data[,target],
## "normalize" column names of covariates to X1, X2, ...
X=normcolnames(data_x),
XY=data.frame(X,Y=data_y), # rename dependent variable as Y
n=nrow(X), # number of rows in the data matrix
k=min(n,10), # number of cross-validation folds
split=kpart(n,k), # the split of n data items into k folds
model=lm, # model to use
## function to train a model on data XY
train=function(XY) model(Y ~ .,data=XY),
## function to make predictions on trained model
pred=function(m,X) predict(m,newdata=X),
## helper function to train a model in datapoints in itr and
### make a prediction on datapoints in iva
f=function(itr,iva) pred(train(XY[itr,,drop=FALSE]),X[iva,,drop=FALSE])) {
## initialize yhat to something of correct data type,
## train model with full data and make a prediction
yhat <- f(1:n,1:n)
if(k>0) {
## go through all folds, train on other folds, and make a prediction
for(iva in split) {
yhat[iva] <- f(setdiff(1:n,iva),iva)
}
}
yhat # finally, output cross-validation predictions
}
## Some regression models implemented in R. For documentation, just type
## ?lm, ?rpart etc. Notice that you need the above mentioned libraries to be
## able to use these models.
models <- list(OLS=lm,
regression_tree=rpart,
random_forest=randomForest,
SVM=svm)
## use the same random split for all - this eliminates variance due
### to different splits for different models
split <- kpart(nrow(data_train),10)
a <- sapply(models,function(model) {
## with k=0 we just get predictions on training data
c(
## with k=0 we just get predictions on training data
train=rmse(cv(data_train,k=0,model=model),data_train[,"medv"]),
## we concatenate test and train data and have them on separate folds,
## after which we get the fold out where
## we trained on training data and predicted on testing data.
## Looks ugly, but
## this allows us to hide all of the details in function cv...
test=rmse(cv(rbind(data_test,data_train),
split=list(1:nrow(data_test),
(nrow(data_test)+1):(nrow(data_test)+nrow(data_train))),
model=model)[1:nrow(data_test)],data_test[,"medv"]),
## we finally do the regular 10-fold cross-validation
CV=rmse(cv(data_train,split=split,model=model),data_train[,"medv"]))
})
knitr::kable(t(a),"simple",digits=3)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
setwd("C:/Users/Topi/IntroToML/Project/Intro_to_ML_project")
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
print(npf)
summary(npf)
set.seed(7)
# load caret
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(class4~., data=npf, method="multinom", preProcess="scale", trControl=control)
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance$rows<-row.names(importance)
importance$Overall <- sort(importance$Overall, decreasing=TRUE)
print(importance[1:30,])
print(importance[,])
library(nnet)
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
npf$class4 <- relevel(npf.class4, ref="nonevent")
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
npf$class4 <- relevel(npf$class4, ref="nonevent")
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
npf$class4 <- relevel(factor(npf$class4), ref="nonevent")
model <- multinom(class4~.,data=npf)
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=(0.8, 0.2))
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=c(0.8, 0.2))
npf_train <- npf[ttsplit==1,]
npf_test <- npf[ttsplit==2,]
print(npf_train)
npf$class4 <- relevel(factor(npf$class4), ref="nonevent") #choose reference level to be noevent
#model <- multinom(class4~.,data=npf)
library(nnet) # multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=c(0.8, 0.2)) #split into test,trait data
npf_train <- npf[ttsplit==1,]
npf_test <- npf[ttsplit==2,]
npf$class4 <- relevel(factor(npf$class4), ref="nonevent") #choose reference level to be noevent
#model <- multinom(class4~.,data=npf)
summary(model)
names(summary(npf_train))
z <- summary(npf_train)$coefficients/summary(npf_train)$standard.errors
names(npf_train)
z <- summary(npf_train)$coefficients/summary(npf_train)$standard.errors
names(summary(npf_train))
z <- summary(npf_train)$coefficients/summary(npf_train)$standard.errors
print(names(summary(npf_train)))
z <- summary(npf_train)$coefficients/summary(npf_train)$standard.errors
z <- summary(npf_train)$Coefficients/summary(npf_train)$Standard.errors
z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
print(typeof(npf_train))
z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
print(is.atomic(npf_train))
z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
print(is.atomic(npf_train))
summary(npf_train)["coefficients"]
#z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
print(is.atomic(npf_train))
summary(npf_train)$coefficients
print(is.atomic(npf_train))
summary(npf_train)
#z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
#print(is.atomic(npf_train))
summary(npf_train)
#z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
library(nnet) #multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=c(0.8, 0.2)) #split into test,trait data
npf_train <- npf[ttsplit==1,]
npf_test <- npf[ttsplit==2,]
npf$class4 <- relevel(factor(npf$class4), ref="nonevent") #choose reference level to be noevent
model <- multinom(class4~.,data=npf_train) #train model
#print(is.atomic(npf_train))
summary(npf_train)
#z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
#print(is.atomic(npf_train))
summary(model)
#z <- summary(npf_train)["coefficients"]/summary(npf_train)["standard.errors"]
print(names(model))
#summary(model)
z <- summary(model)$coefficients/summary(model)$coefficients
z <- summary(model)$coefficients/summary(model)$standard.error
z <- summary(model)$coefficients/summary(model)$standard.error
p <- (1-pnorm(abs(z),0,1))*2 #*2 since we want 2 tailed z-test
p
set.seed(7)
# load the library
library(mlbench)
install.packages("mlbench")
set.seed(7)
# load the library
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train, rfeControl=control)
set.seed(7)
# load the library
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1:length(colnames(npf_train))],
sizes=nvars,
rfeControl=control)
npf_train[,2:length(colnames(npf_train))]
npf_train[,1:length(colnames(npf_train))]
set.seed(7)
# load the library
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train$class4,
sizes=nvars,
rfeControl=control)
npf_train[,2:length(colnames(npf_train))]
set.seed(7)
# load the library
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1],
sizes=nvars,
rfeControl=control)
npf_train[,2:length(colnames(npf_train))]
npf_train[,1]
npf_train[,2:length(colnames(npf_train))]
print(npf_train[,1])
npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
set.seed(7)
# load the library
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1],
sizes=nvars,
rfeControl=control)
knitr::opts_chunk$set(echo = TRUE)
npf <- read.csv("npf_train.csv") #read in data
npf <- npf[,-c(1:2,4)] # drop the id, date, partlybad columns
print(npf)
summary(npf)
set.seed(7)
# load caret
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(class4~., data=npf, method="multinom", preProcess="scale", trControl=control)
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance$rows<-row.names(importance)
importance$Overall <- sort(importance$Overall, decreasing=TRUE)
print(importance[,15])
print(importance[,15])
set.seed(7)
# load caret
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(class4~., data=npf, method="multinom", preProcess="scale", trControl=control)
importance <- data.frame(varImp(model, scale=FALSE)$importance)
importance$rows<-row.names(importance)
importance$Overall <- sort(importance$Overall, decreasing=TRUE)
print(importance[,15])
print(importance[,])
print()
library(nnet) #multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=c(0.8, 0.2)) #split into test,trait data
npf_train <- npf[ttsplit==1,]
npf_test <- npf[ttsplit==2,]
#choose reference level to be noevent and train the model:
model <- multinom(relevel(factor(npf$class4), ref="nonevent")~.,data=npf_train)
library(nnet) #multinomial logistic regression in nnet is used
set.seed=(6)
ttsplit <- sample(2, nrow(npf), replace= TRUE, prob=c(0.8, 0.2)) #split into test,trait data
npf_train <- npf[ttsplit==1,]
npf_test <- npf[ttsplit==2,]
#choose reference level to be noevent and train the model:
#npf$class4 <- relevel(factor(npf$class4), ref="nonevent")
model <- multinom(npf_train$class4~.,data=npf_train)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70,90) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1],
sizes=nvars,
rfeControl=control)
npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
print(npf_train[,1])
#npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
print(npf_train[,1])
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the data
data(PimaIndiansDiabetes)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(PimaIndiansDiabetes[,1:8], PimaIndiansDiabetes[,9], sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#npf_train[,2:length(colnames(npf_train))]
print(npf_train$class4)
print(npf_train[,1])
length(colnames(npf_train))
#npf_train[,2:length(colnames(npf_train))]
length(npf_train[,2:length(colnames(npf_train))])
length(npf_train[,1])
length(colnames(npf_train))
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1],
#sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train[,1],
sizes=nvars,
rfeControl=control)
#npf_train[,2:length(colnames(npf_train))]
length(npf_train[,2:length(colnames(npf_train))])
length(npf_train[,1])
length(colnames(npf_train))
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:10],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[1,2],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[1,2:10],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))-1],
npf_train$class4,
sizes=nvars,
rfeControl=control)
library(mlbench)
nvars <- c(1:5,10,15,20,30,50,70) # how many predictors we want to test with
# define the control using a linear model selection function lm
control <- rfeControl(functions=lmFuncs, method="cv", number=10, verbose=FALSE)
results <- rfe(npf_train[,2:length(colnames(npf_train))],
npf_train$class4,
sizes=nvars,
rfeControl=control)
length(npf_train[,2:length(colnames(npf_train))])
length(npf$class4)
length(colnames(npf_train))
length(npf_train[,2:length(colnames(npf_train))])
length(npf_train$class4)
length(colnames(npf_train))
dim(npf_train[,2:length(colnames(npf_train))])
length(npf_train$class4)
length(colnames(npf_train))
dim(npf_train[,2:length(colnames(npf_train))])
dim(npf_train$class4)
length(colnames(npf_train))
dim(npf_train[,2:length(colnames(npf_train))])
length(npf_train$class4)
length(colnames(npf_train))
